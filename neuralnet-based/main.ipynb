{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 15:15:44.343697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-16 15:15:44.664191: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-16 15:15:45.896059: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 15:15:45.896761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 15:15:45.896768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.loadtxt(\"words100.txt\", dtype=str, delimiter=\" \", encoding=\"utf-8\")\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsDataset(tf.keras.utils.Sequence):\n",
    "    padding_char = \" \"\n",
    "\n",
    "    def __init__(self, phase, words, batch_size = 100, alphabet = list(string.ascii_lowercase)):\n",
    "        self.phase = phase\n",
    "        self.words = copy.deepcopy(words)\n",
    "        self.batch_size = batch_size\n",
    "        self.alphabet = alphabet\n",
    "        self.one_hot_encoding_model = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "                tf.keras.layers.TextVectorization(\n",
    "                    output_mode=\"multi_hot\",\n",
    "                    vocabulary=self.alphabet)\n",
    "            ]\n",
    "        )\n",
    "        self.longest_word = max(words, key=len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.words) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_rand = self.words[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.words[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        # x_prim = self.change_random_char(x_prim)\n",
    "        # x_prim = self.encode(x_prim)\n",
    "        x_rand = np.array([\n",
    "            self.encode(self.change_random_char(x)) for x in batch_x_rand\n",
    "        ])\n",
    "        x = np.array([\n",
    "            self.encode(x) for x in batch_x\n",
    "        ])\n",
    "        return x_rand, x\n",
    "    \n",
    "    def __call__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "            \n",
    "            if i == self.__len__()-1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    # shuffles the dataset at the end of each epoch\n",
    "    def on_epoch_end(self):\n",
    "        reidx = random.sample(population = list(range(self.__len__())),k = self.__len__())\n",
    "        self.words = self.words[reidx]\n",
    "\n",
    "    def random_char(self, exclude):\n",
    "        char = random.choice(self.alphabet)\n",
    "        return self.random_char(exclude) if char == exclude else char\n",
    "    \n",
    "    def change_random_char(self, word):\n",
    "        idx = random.randint(0, len(word) - 1)\n",
    "        char = self.random_char(word[idx])\n",
    "        return word[:idx] + char + word[idx+1:]\n",
    "\n",
    "    # splits word into list of characters\n",
    "    def split_word(self, x):\n",
    "        return list(x)\n",
    "\n",
    "    # pads \n",
    "    def pad(self, x):\n",
    "        for _ in range(len(self.longest_word) - len(x)):\n",
    "            x.append(self.padding_char)\n",
    "        return x\n",
    "    \n",
    "    # performs one-hot encoding on x\n",
    "    def encode(self, x):\n",
    "        x = self.split_word(x)\n",
    "        x = self.pad(x)\n",
    "        x = np.array(self.one_hot_encoding_model.predict(x, verbose=0).reshape(1,-1))\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = x.reshape(len(self.longest_word), len(self.alphabet) + 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = WordsDataset(\"train\", words, batch_size=32)\n",
    "validation_generator = WordsDataset(\"validation\", words, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word: destruction\n",
      "Longest word length: 11\n"
     ]
    }
   ],
   "source": [
    "longest_word = max(words, key=len)\n",
    "print(f\"Longest word: {longest_word}\")\n",
    "print(f\"Longest word length: {len(longest_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(string.ascii_lowercase)\n",
    "print(alphabet)\n",
    "print(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/output size = 297\n"
     ]
    }
   ],
   "source": [
    "output_size = (len(string.ascii_lowercase) + 1) * len(longest_word)\n",
    "print(f\"Input/output size = {output_size}\")\n",
    "\n",
    "latent_dim = 30\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(output_size),\n",
    "            tf.keras.layers.Dense(latent_dim, activation=\"relu\")\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(output_size, activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "\n",
    "# autoencoder = tf.keras.models.Sequential(\n",
    "#     [\n",
    "#         tf.keras.layers.Input(shape=(input_size,)),\n",
    "#         tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
    "#         tf.keras.layers.Dense(input_size, activation=\"sigmoid\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2764 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.2764 - binary_accuracy: 1.0000 - val_loss: 0.4487 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2725 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2725 - binary_accuracy: 1.0000 - val_loss: 0.4407 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2451 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.2451 - binary_accuracy: 1.0000 - val_loss: 0.4234 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2373 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.2373 - binary_accuracy: 1.0000 - val_loss: 0.4323 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2153 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.2153 - binary_accuracy: 1.0000 - val_loss: 0.4301 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2567 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2567 - binary_accuracy: 1.0000 - val_loss: 0.4116 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2381 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2381 - binary_accuracy: 1.0000 - val_loss: 0.4083 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2290 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2290 - binary_accuracy: 1.0000 - val_loss: 0.4226 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2145 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2145 - binary_accuracy: 1.0000 - val_loss: 0.4890 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2182 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2182 - binary_accuracy: 1.0000 - val_loss: 0.4799 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1931 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1931 - binary_accuracy: 1.0000 - val_loss: 0.4820 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2222 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.2222 - binary_accuracy: 1.0000 - val_loss: 0.3989 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1716 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1716 - binary_accuracy: 1.0000 - val_loss: 0.3890 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1978 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1978 - binary_accuracy: 1.0000 - val_loss: 0.4029 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2042 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.2042 - binary_accuracy: 1.0000 - val_loss: 0.4021 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2023 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2023 - binary_accuracy: 1.0000 - val_loss: 0.5039 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1913 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1913 - binary_accuracy: 1.0000 - val_loss: 0.3775 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1600 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1600 - binary_accuracy: 1.0000 - val_loss: 0.4068 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1487 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1487 - binary_accuracy: 1.0000 - val_loss: 0.4372 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1763 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1763 - binary_accuracy: 1.0000 - val_loss: 0.4415 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1787 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1787 - binary_accuracy: 1.0000 - val_loss: 0.3603 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1594 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1594 - binary_accuracy: 1.0000 - val_loss: 0.3559 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1457 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1457 - binary_accuracy: 1.0000 - val_loss: 0.4629 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1350 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.1350 - binary_accuracy: 1.0000 - val_loss: 0.3393 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1306 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1306 - binary_accuracy: 1.0000 - val_loss: 0.3309 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1243 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1243 - binary_accuracy: 1.0000 - val_loss: 0.3575 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1271 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1271 - binary_accuracy: 1.0000 - val_loss: 0.3384 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1331 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1331 - binary_accuracy: 1.0000 - val_loss: 0.3453 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1139 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1139 - binary_accuracy: 1.0000 - val_loss: 0.3534 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1229 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1229 - binary_accuracy: 1.0000 - val_loss: 0.4339 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1195 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1195 - binary_accuracy: 1.0000 - val_loss: 0.3196 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1243 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1243 - binary_accuracy: 1.0000 - val_loss: 0.3401 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1168 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1168 - binary_accuracy: 1.0000 - val_loss: 0.3141 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0948 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0948 - binary_accuracy: 1.0000 - val_loss: 0.3199 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0933 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0933 - binary_accuracy: 1.0000 - val_loss: 0.3129 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0914 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0914 - binary_accuracy: 1.0000 - val_loss: 0.3077 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0907 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0907 - binary_accuracy: 1.0000 - val_loss: 0.3122 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0878 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0878 - binary_accuracy: 1.0000 - val_loss: 0.2982 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0954 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0954 - binary_accuracy: 1.0000 - val_loss: 0.3203 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0907 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0907 - binary_accuracy: 1.0000 - val_loss: 0.4173 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0737 - binary_accuracy: 1.0000 - val_loss: 0.2776 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0796 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0796 - binary_accuracy: 1.0000 - val_loss: 0.3019 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0829 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0829 - binary_accuracy: 1.0000 - val_loss: 0.2793 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0780 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0780 - binary_accuracy: 1.0000 - val_loss: 0.2703 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0868 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0868 - binary_accuracy: 1.0000 - val_loss: 0.4004 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0679 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0679 - binary_accuracy: 1.0000 - val_loss: 0.3038 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0692 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0692 - binary_accuracy: 1.0000 - val_loss: 0.2956 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0729 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0729 - binary_accuracy: 1.0000 - val_loss: 0.1899 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0728 - binary_accuracy: 1.0000 - val_loss: 0.2849 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0587 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0587 - binary_accuracy: 1.0000 - val_loss: 0.2654 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0597 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0597 - binary_accuracy: 1.0000 - val_loss: 0.2827 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0545 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0545 - binary_accuracy: 1.0000 - val_loss: 0.3904 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0727 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0727 - binary_accuracy: 1.0000 - val_loss: 0.2618 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0755 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0755 - binary_accuracy: 1.0000 - val_loss: 0.2152 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0617 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0617 - binary_accuracy: 1.0000 - val_loss: 0.2557 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0537 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0537 - binary_accuracy: 1.0000 - val_loss: 0.2628 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0650 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0650 - binary_accuracy: 1.0000 - val_loss: 0.2630 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0583 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0583 - binary_accuracy: 1.0000 - val_loss: 0.2299 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0612 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0612 - binary_accuracy: 1.0000 - val_loss: 0.2522 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0563 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0563 - binary_accuracy: 1.0000 - val_loss: 0.2534 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0483 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0483 - binary_accuracy: 1.0000 - val_loss: 0.2571 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0535 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0535 - binary_accuracy: 1.0000 - val_loss: 0.2416 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0484 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0484 - binary_accuracy: 1.0000 - val_loss: 0.2355 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0482 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0482 - binary_accuracy: 1.0000 - val_loss: 0.2328 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0499 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0499 - binary_accuracy: 1.0000 - val_loss: 0.2249 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0419 - binary_accuracy: 1.0000 - val_loss: 0.3690 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0372 - binary_accuracy: 1.0000 - val_loss: 0.2226 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0535 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0535 - binary_accuracy: 1.0000 - val_loss: 0.2270 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0483 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0483 - binary_accuracy: 1.0000 - val_loss: 0.2341 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0434 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0434 - binary_accuracy: 1.0000 - val_loss: 0.2440 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0349 - binary_accuracy: 1.0000 - val_loss: 0.2213 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0345 - binary_accuracy: 1.0000 - val_loss: 0.2305 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0423 - binary_accuracy: 1.0000 - val_loss: 0.2100 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0383 - binary_accuracy: 1.0000 - val_loss: 0.3560 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0323 - binary_accuracy: 1.0000 - val_loss: 0.2297 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0405 - binary_accuracy: 1.0000 - val_loss: 0.3023 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0290 - binary_accuracy: 1.0000 - val_loss: 0.2344 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0402 - binary_accuracy: 1.0000 - val_loss: 0.2027 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0320 - binary_accuracy: 1.0000 - val_loss: 0.2235 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0369 - binary_accuracy: 1.0000 - val_loss: 0.2394 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0302 - binary_accuracy: 1.0000 - val_loss: 0.2145 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0242 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0242 - binary_accuracy: 1.0000 - val_loss: 0.3186 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0236 - binary_accuracy: 1.0000 - val_loss: 0.2088 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0293 - binary_accuracy: 1.0000 - val_loss: 0.2363 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0332 - binary_accuracy: 1.0000 - val_loss: 0.3139 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0246 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0246 - binary_accuracy: 1.0000 - val_loss: 0.2053 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0317 - binary_accuracy: 1.0000 - val_loss: 0.1986 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0336 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0336 - binary_accuracy: 1.0000 - val_loss: 0.3115 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0231 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0231 - binary_accuracy: 1.0000 - val_loss: 0.3283 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0268 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0268 - binary_accuracy: 1.0000 - val_loss: 0.2184 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0221 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0221 - binary_accuracy: 1.0000 - val_loss: 0.2162 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0247 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0247 - binary_accuracy: 1.0000 - val_loss: 0.2048 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0212 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0212 - binary_accuracy: 1.0000 - val_loss: 0.1930 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0298 - binary_accuracy: 1.0000 - val_loss: 0.1945 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0293 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0293 - binary_accuracy: 1.0000 - val_loss: 0.1957 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0203 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0203 - binary_accuracy: 1.0000 - val_loss: 0.2830 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0187 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0187 - binary_accuracy: 1.0000 - val_loss: 0.1865 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0271 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0271 - binary_accuracy: 1.0000 - val_loss: 0.1992 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0236 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0236 - binary_accuracy: 1.0000 - val_loss: 0.1981 - val_binary_accuracy: 0.9764\n",
      "['after']\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0280 - binary_accuracy: 1.0000['page']\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0280 - binary_accuracy: 1.0000 - val_loss: 0.2024 - val_binary_accuracy: 0.9764\n",
      "['after']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1b5be0bb0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "autoencoder.fit(\n",
    "    training_generator,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "end_encoded = training_generator.encode(\"end\")\n",
    "end_encoded_reshaped = end_encoded.reshape(-1,)\n",
    "end_decoded = training_generator.decode(end_encoded)\n",
    "# lookup = tf.keras.layers.StringLookup(vocabulary=training_generator.one_hot_encoding_model.get_layer(\"text_vectorization_6\").get_vocabulary(), invert=True)\n",
    "# vocab = training_generator.one_hot_encoding_model.get_layer(\"text_vectorization_6\").get_vocabulary()\n",
    "# print(vocab)\n",
    "print(end_encoded)\n",
    "# print(end_encoded_reshaped)\n",
    "# print(end_decoded)\n",
    "# print(end_encoded==end_decoded)\n",
    "# print(lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.24785538 0.7464843  0.23704754 0.28257364 0.23298262 0.2643738\n",
      "  0.2604562  0.25370482 0.2634375  0.23266163 0.26129532 0.25558382\n",
      "  0.2724916  0.22224587 0.2859145  0.25303578 0.22898032 0.27738267\n",
      "  0.24465336 0.24342085 0.26277605 0.227334   0.2854598  0.2613166\n",
      "  0.27125108 0.25013727 0.24037239 0.25242922 0.25554422 0.2521487\n",
      "  0.24242331 0.25304204 0.25296202 0.72087216 0.25176314 0.2450378\n",
      "  0.24288566 0.26151878 0.2637278  0.2215964  0.2575819  0.2121628\n",
      "  0.2526189  0.25425357 0.254407   0.26619726 0.27523348 0.2424623\n",
      "  0.2410545  0.2598795  0.26659772 0.2045465  0.27393317 0.21071509\n",
      "  0.25972825 0.25095633 0.26119855 0.2421889  0.2751087  0.24005279\n",
      "  0.25717932 0.2045456  0.26519296 0.24588034 0.2698602  0.23477662\n",
      "  0.26288158 0.26720518 0.24796736 0.22912897 0.27689356 0.2527421\n",
      "  0.26855356 0.24849445 0.7595825  0.25248688 0.23844019 0.24783733\n",
      "  0.22396268 0.2504417  0.27177778 0.2491801  0.2456874  0.22381173\n",
      "  0.24825448 0.27242923 0.77221584 0.22967567 0.251034   0.2165087\n",
      "  0.2249111  0.21722502 0.23651284 0.2407097  0.25903147 0.284812\n",
      "  0.25939152 0.25805867 0.27692908 0.24782091 0.24108477 0.23614153\n",
      "  0.2603813  0.26102933 0.26174417 0.25770208 0.26009053 0.25336263\n",
      "  0.25693    0.2610329  0.238016   0.23997933 0.2696192  0.29249594\n",
      "  0.21379718 0.26813403 0.28285787 0.2715339  0.25968406 0.27131516\n",
      "  0.25991142 0.25479382 0.27640727 0.2445019  0.23827466 0.27446112\n",
      "  0.74830264 0.25214854 0.26995057 0.26264668 0.24387479 0.251644\n",
      "  0.2271794  0.22883141 0.25475484 0.28446862 0.2542561  0.26252118\n",
      "  0.24271637 0.25935182 0.23809576 0.22913139 0.23798063 0.25041702\n",
      "  0.2356708  0.25041497 0.21265331 0.26616463 0.23396493 0.2429081\n",
      "  0.27322346 0.23024474 0.2639505  0.2426807  0.2973908  0.22459695\n",
      "  0.26726776 0.26197007 0.2445632  0.27935827 0.25638628 0.2611907\n",
      "  0.23069522 0.2489729  0.23066261 0.24640967 0.23294486 0.26443657\n",
      "  0.23994292 0.27003855 0.2606284  0.2215236  0.2445832  0.25360382\n",
      "  0.26821476 0.26046032 0.25306398 0.25779563 0.25321817 0.24121436\n",
      "  0.28549513 0.22396518 0.22508751 0.25828612 0.23900878 0.2783114\n",
      "  0.2641555  0.2667101  0.25990945 0.23657012 0.23816536 0.28106797\n",
      "  0.22649616 0.26071185 0.28096083 0.28942698 0.25012878 0.26016304\n",
      "  0.25321972 0.26760146 0.2653506  0.27686477 0.25540373 0.26014093\n",
      "  0.23063718 0.24947171 0.22011812 0.24332133 0.23970401 0.24504305\n",
      "  0.23264548 0.2590821  0.24578331 0.2556429  0.2458333  0.2540654\n",
      "  0.24455212 0.29600695 0.29280457 0.2442401  0.22770756 0.26577196\n",
      "  0.2403568  0.25264016 0.25248265 0.23934466 0.24245171 0.24998683\n",
      "  0.2619847  0.25616285 0.26023692 0.22828092 0.2594526  0.28703222\n",
      "  0.22290948 0.25027838 0.2432692  0.22341879 0.24884725 0.23642653\n",
      "  0.2557883  0.21801141 0.25061214 0.23071791 0.2232278  0.25006834\n",
      "  0.21046758 0.25402427 0.26584145 0.24520768 0.23472364 0.21957062\n",
      "  0.28170517 0.25243333 0.30157748 0.26687264 0.27664447 0.26865414\n",
      "  0.25267434 0.29226243 0.2628451  0.2757976  0.24832444 0.23705263\n",
      "  0.22635806 0.23086955 0.24994673 0.24647573 0.25013557 0.23474172\n",
      "  0.26418695 0.28352138 0.22917621 0.27544823 0.22483793 0.24962758\n",
      "  0.272781   0.23122974 0.24756385 0.23355484 0.26697728 0.25589156\n",
      "  0.2653701  0.27414203 0.23378365 0.25800064 0.25618312 0.26003602\n",
      "  0.24325788 0.26304263 0.26794606 0.26410887 0.23577785 0.28093883\n",
      "  0.24330609 0.2716161  0.2498762 ]]\n"
     ]
    }
   ],
   "source": [
    "prediction = autoencoder.predict(training_generator.encode(\"farts\"))\n",
    "print(prediction)\n",
    "# autoencoder.summary()\n",
    "# autoencoder.predict(training_generator.encode(\"farts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = []\n",
    "# y_train = []\n",
    "\n",
    "# x_test = []\n",
    "# y_test = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     for x, y in training_generator:\n",
    "#         x_train.append(x)\n",
    "#         y_train.append(y)\n",
    "\n",
    "# for i in range(2):\n",
    "#     for x, y in validation_generator:\n",
    "#         x_test.append(x)\n",
    "#         y_test.append(y)\n",
    "\n",
    "# x_train = np.asarray(x_train)\n",
    "# y_train = np.asarray(y_train)\n",
    "# x_test = np.asarray(x_test)\n",
    "# y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "#     num_words=len(alphabet),\n",
    "#     oov_token=\" \",\n",
    "#     char_level=True\n",
    "# )\n",
    "# char_tokenizer.fit_on_texts(alphabet)\n",
    "\n",
    "# sequences = char_tokenizer.texts_to_sequences(alphabet)\n",
    "\n",
    "# one_hot_chars = char_tokenizer.texts_to_matrix(alphabet, mode=\"binary\")\n",
    "\n",
    "# char_index = char_tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(char_index))\n",
    "# print(char_index)\n",
    "\n",
    "# print(char_tokenizer.texts_to_matrix(words[0], mode=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoding_model = tf.keras.models.Sequential(\n",
    "#     [\n",
    "#         tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "#         tf.keras.layers.TextVectorization(\n",
    "#             output_mode=\"multi_hot\",\n",
    "#             vocabulary=alphabet\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "# one_hot_encoded = np.array([one_hot_encoding_model.predict(word).reshape(-1,) for word in padded_chars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encoded = np.array([one_hot_encoding_model.predict(word).reshape(-1,) for word in padded_chars])\n",
    "# print(len(one_hot_encoded[0]))\n",
    "# print(words[0])\n",
    "# print(one_hot_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.Sequential(\n",
    "#             [\n",
    "#                 tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "#                 tf.keras.layers.TextVectorization(\n",
    "#                     output_mode=\"multi_hot\",\n",
    "#                     vocabulary=self.alphabet)\n",
    "#             ]\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86671bdf1840f33d82b42430fbcbd01be7d8dd5bccd7f83f514ba6d9fa11e513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
